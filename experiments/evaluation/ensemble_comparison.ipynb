{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Comparison: HYDRA + QUANT Stacking\n",
    "\n",
    "**Objective**: Compare the new clean stacked ensemble (with proper cross-validation) against:\n",
    "- Individual QUANT and HYDRA algorithms\n",
    "- Old stacked ensemble (with data leakage)\n",
    "\n",
    "**Methodology**:\n",
    "- Multiple hyperparameter configurations\n",
    "- Statistical analysis and comparison\n",
    "- Runtime and accuracy metrics\n",
    "\n",
    "**Dataset**: Configurable (default: Pedestrian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded: Pedestrian dataset\n",
      "Training: 10.0%, Testing: 50.0%\n",
      "Testing 2 hyperparameter configurations\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION\n",
    "DATASET_NAME = \"Pedestrian\"  # Change this to test other datasets\n",
    "TRAIN_PCT = 10.0             # Percentage of training data (0-100 scale)\n",
    "TEST_PCT = 50.0              # Percentage of test data (0-100 scale)\n",
    "RANDOM_SEED = 42             # For reproducibility\n",
    "\n",
    "# Hyperparameter configurations to test\n",
    "CONFIGS = {\n",
    "    \"fast\": {\n",
    "        \"hydra_k\": 4,\n",
    "        \"hydra_g\": 16,\n",
    "        \"n_estimators\": 50,\n",
    "        \"n_folds\": 3\n",
    "    },\n",
    "    \"balanced\": {\n",
    "        \"hydra_k\": 8,\n",
    "        \"hydra_g\": 64,\n",
    "        \"n_estimators\": 100,\n",
    "        \"n_folds\": 5\n",
    "    }\n",
    "    # Note: Removed \"accurate\" config as it's too slow for testing\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded: {DATASET_NAME} dataset\")\n",
    "print(f\"Training: {TRAIN_PCT}%, Testing: {TEST_PCT}%\")\n",
    "print(f\"Testing {len(CONFIGS)} hyperparameter configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries and paths configured\n"
     ]
    }
   ],
   "source": [
    "# Setup paths BEFORE importing tsckit\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "sys.path.extend([\n",
    "    '/Users/urav/code/research',\n",
    "    '/Users/urav/code/research/quant/code',\n",
    "    '/Users/urav/code/research/hydra/code',\n",
    "    '/Users/urav/code/research/aaltd2024/code',\n",
    "])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Libraries and paths configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/urav/code/research/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Algorithm imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import algorithms\n",
    "from tsckit import (\n",
    "    MonsterDataset,\n",
    "    QuantAALTD2024,\n",
    "    HydraAALTD2024,\n",
    "    HydraQuantStackedAALTD2024,  # Old ensemble (data leakage)\n",
    "    HydraQuantStacked            # New clean ensemble\n",
    ")\n",
    "\n",
    "print(\"✓ Algorithm imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pedestrian dataset...\n",
      "Pedestrian (fold 0):\n",
      "  Shape: 1 channels x 24 time points\n",
      "  Classes: 82\n",
      "  Total samples: 189621\n",
      "  Train samples: 15169 (10.0%)\n",
      "  Test samples: 18962 (50.0%)\n",
      "Test samples: 18962, Classes: 82\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(f\"Loading {DATASET_NAME} dataset...\")\n",
    "dataset = MonsterDataset(DATASET_NAME, fold=0, train_pct=TRAIN_PCT, test_pct=TEST_PCT)\n",
    "print(dataset.info())\n",
    "\n",
    "# Get ground truth for accuracy calculation\n",
    "_, y_test = dataset.get_arrays(\"test\")\n",
    "print(f\"Test samples: {len(y_test)}, Classes: {len(np.unique(y_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "# Helper function to run and time algorithms\n",
    "def evaluate_algorithm(algorithm, name: str, config_name: str) -> Dict:\n",
    "    \"\"\"Run algorithm and return performance metrics.\"\"\"\n",
    "    print(f\"\\n🔄 Testing {name} ({config_name})...\")\n",
    "    \n",
    "    try:\n",
    "        # Training\n",
    "        train_start = time.time()\n",
    "        algorithm.fit(dataset)\n",
    "        train_time = time.time() - train_start\n",
    "        \n",
    "        # Testing\n",
    "        test_start = time.time()\n",
    "        predictions = algorithm.predict(dataset)\n",
    "        test_time = time.time() - test_start\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = np.mean(predictions == y_test)\n",
    "        \n",
    "        print(f\"   ✅ {name}: {accuracy:.4f} accuracy, {train_time:.2f}s train, {test_time:.2f}s test\")\n",
    "        \n",
    "        return {\n",
    "            \"algorithm\": name,\n",
    "            \"config\": config_name,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"train_time\": train_time,\n",
    "            \"test_time\": test_time,\n",
    "            \"total_time\": train_time + test_time,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ {name} FAILED: {str(e)}\")\n",
    "        return {\n",
    "            \"algorithm\": name,\n",
    "            \"config\": config_name,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"train_time\": 0.0,\n",
    "            \"test_time\": 0.0,\n",
    "            \"total_time\": 0.0,\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"✓ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive evaluation...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "📋 Configuration: FAST\n",
      "   HYDRA: k=4, g=16\n",
      "   QUANT: estimators=50\n",
      "   Ensemble: folds=3\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔄 Testing QuantAALTD2024 (fast)...\n",
      "   ✅ QuantAALTD2024: 0.6845 accuracy, 0.78s train, 0.21s test\n",
      "\n",
      "🔄 Testing HydraAALTD2024 (fast)...\n",
      "   ✅ HydraAALTD2024: 0.4501 accuracy, 0.85s train, 1.17s test\n",
      "\n",
      "🔄 Testing OldEnsemble (fast)...\n",
      "   ✅ OldEnsemble: 0.6808 accuracy, 2.30s train, 0.94s test\n",
      "\n",
      "🔄 Testing NewEnsemble (fast)...\n",
      "Training ensemble: 15169 samples, 82 classes, 24 length\n",
      "Processing fold 1/3\n",
      "Fold 1: train=10112, val=5057, logits_shape=(5057, 82)\n",
      "Processing fold 2/3\n",
      "Fold 2: train=10113, val=5056, logits_shape=(5056, 82)\n",
      "Processing fold 3/3\n",
      "Fold 3: train=10113, val=5056, logits_shape=(5056, 82)\n",
      "Training final HYDRA model on full dataset\n",
      "Extracting QUANT features\n",
      "Features: QUANT=(15169, 212), OOF_logits=(15169, 82)\n",
      "Training meta-classifier: features=(15169, 294), labels=(15169,)\n",
      "   ✅ NewEnsemble: 0.6840 accuracy, 3.46s train, 1.10s test\n",
      "\n",
      "📋 Configuration: BALANCED\n",
      "   HYDRA: k=8, g=64\n",
      "   QUANT: estimators=100\n",
      "   Ensemble: folds=5\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔄 Testing QuantAALTD2024 (balanced)...\n",
      "   ✅ QuantAALTD2024: 0.6960 accuracy, 1.21s train, 0.31s test\n",
      "\n",
      "🔄 Testing HydraAALTD2024 (balanced)...\n",
      "   ✅ HydraAALTD2024: 0.5706 accuracy, 5.14s train, 200.67s test\n",
      "\n",
      "🔄 Testing OldEnsemble (balanced)...\n",
      "   ✅ OldEnsemble: 0.7006 accuracy, 10.57s train, 4.65s test\n",
      "\n",
      "🔄 Testing NewEnsemble (balanced)...\n",
      "Training ensemble: 15169 samples, 82 classes, 24 length\n",
      "Processing fold 1/5\n",
      "Fold 1: train=12135, val=3034, logits_shape=(3034, 82)\n",
      "Processing fold 2/5\n",
      "Fold 2: train=12135, val=3034, logits_shape=(3034, 82)\n",
      "Processing fold 3/5\n",
      "Fold 3: train=12135, val=3034, logits_shape=(3034, 82)\n",
      "Processing fold 4/5\n",
      "Fold 4: train=12135, val=3034, logits_shape=(3034, 82)\n",
      "Processing fold 5/5\n",
      "Fold 5: train=12136, val=3033, logits_shape=(3033, 82)\n",
      "Training final HYDRA model on full dataset\n",
      "Extracting QUANT features\n",
      "Features: QUANT=(15169, 212), OOF_logits=(15169, 82)\n",
      "Training meta-classifier: features=(15169, 294), labels=(15169,)\n",
      "   ✅ NewEnsemble: 0.6947 accuracy, 29.37s train, 567.11s test\n",
      "\n",
      "🎯 Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Run comprehensive evaluation\n",
    "results = []\n",
    "\n",
    "print(\"🚀 Starting comprehensive evaluation...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config_name, config in CONFIGS.items():\n",
    "    print(f\"\\n📋 Configuration: {config_name.upper()}\")\n",
    "    print(f\"   HYDRA: k={config['hydra_k']}, g={config['hydra_g']}\")\n",
    "    print(f\"   QUANT: estimators={config['n_estimators']}\")\n",
    "    print(f\"   Ensemble: folds={config['n_folds']}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Individual algorithms\n",
    "    quant = QuantAALTD2024(num_estimators=config['n_estimators'])\n",
    "    results.append(evaluate_algorithm(quant, \"QuantAALTD2024\", config_name))\n",
    "    \n",
    "    hydra = HydraAALTD2024(k=config['hydra_k'], g=config['hydra_g'], seed=RANDOM_SEED)\n",
    "    results.append(evaluate_algorithm(hydra, \"HydraAALTD2024\", config_name))\n",
    "    \n",
    "    # Old ensemble (data leakage)\n",
    "    old_ensemble = HydraQuantStackedAALTD2024(\n",
    "        hydra_k=config['hydra_k'],\n",
    "        hydra_g=config['hydra_g'],\n",
    "        hydra_seed=RANDOM_SEED,\n",
    "        quant_estimators=config['n_estimators']\n",
    "    )\n",
    "    results.append(evaluate_algorithm(old_ensemble, \"OldEnsemble\", config_name))\n",
    "    \n",
    "    # New clean ensemble\n",
    "    new_ensemble = HydraQuantStacked(\n",
    "        n_folds=config['n_folds'],\n",
    "        hydra_k=config['hydra_k'],\n",
    "        hydra_g=config['hydra_g'],\n",
    "        hydra_seed=RANDOM_SEED,\n",
    "        n_estimators=config['n_estimators']\n",
    "    )\n",
    "    results.append(evaluate_algorithm(new_ensemble, \"NewEnsemble\", config_name))\n",
    "\n",
    "print(\"\\n🎯 Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "🏆 PERFORMANCE BY ALGORITHM:\n",
      "QuantAALTD2024      : 0.6902 ± 0.0081 (max: 0.6960) | 1.3s avg\n",
      "HydraAALTD2024      : 0.5103 ± 0.0852 (max: 0.5706) | 103.9s avg\n",
      "OldEnsemble         : 0.6907 ± 0.0140 (max: 0.7006) | 9.2s avg\n",
      "NewEnsemble         : 0.6893 ± 0.0075 (max: 0.6947) | 300.5s avg\n",
      "\n",
      "🎯 BEST CONFIGURATION PER ALGORITHM:\n",
      "QuantAALTD2024      : balanced config → 0.6960\n",
      "HydraAALTD2024      : balanced config → 0.5706\n",
      "OldEnsemble         : balanced config → 0.7006\n",
      "NewEnsemble         : balanced config → 0.6947\n",
      "\n",
      "🔥 ENSEMBLE COMPARISON:\n",
      "QuantAALTD2024      : 0.6960\n",
      "HydraAALTD2024      : 0.5706\n",
      "OldEnsemble         : 0.7006\n",
      "NewEnsemble         : 0.6947\n",
      "\n",
      "🏆 OVERALL WINNER: OldEnsemble (balanced) → 0.7006\n",
      "\n",
      "📋 DETAILED RESULTS:\n",
      "--------------------------------------------------------------------------------\n",
      "     algorithm   config  accuracy  train_time  test_time  status\n",
      "QuantAALTD2024     fast  0.684474    0.780864   0.212464 success\n",
      "HydraAALTD2024     fast  0.450058    0.848194   1.172536 success\n",
      "   OldEnsemble     fast  0.680783    2.301578   0.935729 success\n",
      "   NewEnsemble     fast  0.684000    3.461863   1.103563 success\n",
      "QuantAALTD2024 balanced  0.695971    1.211902   0.305452 success\n",
      "HydraAALTD2024 balanced  0.570615    5.143761 200.668143 success\n",
      "   OldEnsemble balanced  0.700612   10.574590   4.647152 success\n",
      "   NewEnsemble balanced  0.694652   29.373251 567.112537 success\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "df = pd.DataFrame(results)\n",
    "successful_df = df[df['status'] == 'success'].copy()\n",
    "\n",
    "print(\"📊 RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(successful_df) == 0:\n",
    "    print(\"❌ No successful runs to analyze!\")\n",
    "else:\n",
    "    # Summary by algorithm\n",
    "    print(\"\\n🏆 PERFORMANCE BY ALGORITHM:\")\n",
    "    summary = successful_df.groupby('algorithm').agg({\n",
    "        'accuracy': ['mean', 'std', 'max'],\n",
    "        'total_time': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    for algo in successful_df['algorithm'].unique():\n",
    "        algo_data = successful_df[successful_df['algorithm'] == algo]\n",
    "        acc_mean = algo_data['accuracy'].mean()\n",
    "        acc_std = algo_data['accuracy'].std()\n",
    "        acc_max = algo_data['accuracy'].max()\n",
    "        time_mean = algo_data['total_time'].mean()\n",
    "        \n",
    "        print(f\"{algo:20s}: {acc_mean:.4f} ± {acc_std:.4f} (max: {acc_max:.4f}) | {time_mean:.1f}s avg\")\n",
    "    \n",
    "    # Best configuration per algorithm\n",
    "    print(\"\\n🎯 BEST CONFIGURATION PER ALGORITHM:\")\n",
    "    for algo in successful_df['algorithm'].unique():\n",
    "        algo_data = successful_df[successful_df['algorithm'] == algo]\n",
    "        best_idx = algo_data['accuracy'].idxmax()\n",
    "        best_row = algo_data.loc[best_idx]\n",
    "        print(f\"{algo:20s}: {best_row['config']} config → {best_row['accuracy']:.4f}\")\n",
    "    \n",
    "    # Ensemble comparison\n",
    "    print(\"\\n🔥 ENSEMBLE COMPARISON:\")\n",
    "    ensemble_algos = ['QuantAALTD2024', 'HydraAALTD2024', 'OldEnsemble', 'NewEnsemble']\n",
    "    for algo in ensemble_algos:\n",
    "        if algo in successful_df['algorithm'].values:\n",
    "            best_acc = successful_df[successful_df['algorithm'] == algo]['accuracy'].max()\n",
    "            print(f\"{algo:20s}: {best_acc:.4f}\")\n",
    "    \n",
    "    # Overall winner\n",
    "    best_overall = successful_df.loc[successful_df['accuracy'].idxmax()]\n",
    "    print(f\"\\n🏆 OVERALL WINNER: {best_overall['algorithm']} ({best_overall['config']}) → {best_overall['accuracy']:.4f}\")\n",
    "\n",
    "# Show detailed results table\n",
    "print(\"\\n📋 DETAILED RESULTS:\")\n",
    "print(\"-\" * 80)\n",
    "display_df = df[['algorithm', 'config', 'accuracy', 'train_time', 'test_time', 'status']].copy()\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 ANALYSIS & INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "📈 Ensemble Effectiveness:\n",
      "   Best individual algorithm: 0.6960\n",
      "   Old ensemble (data leak):  0.7006 (+0.0046)\n",
      "   New ensemble (clean CV):   0.6947 (-0.0013)\n",
      "   ⚠️  Old ensemble still ahead by 0.0060 (but has data leakage)\n",
      "\n",
      "⏱️  Runtime Analysis:\n",
      "   HydraAALTD2024      : 103.9 ± 144.1 seconds\n",
      "   NewEnsemble         : 300.5 ± 418.6 seconds\n",
      "   OldEnsemble         : 9.2 ± 8.5 seconds\n",
      "   QuantAALTD2024      : 1.3 ± 0.4 seconds\n",
      "\n",
      "⚙️  Configuration Insights:\n",
      "   balanced  : 0.6655 ± 0.0633 average accuracy\n",
      "   fast      : 0.6248 ± 0.1165 average accuracy\n",
      "\n",
      "✅ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Analysis and insights\n",
    "print(\"\\n🔍 ANALYSIS & INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(successful_df) > 0:\n",
    "    # Ensemble effectiveness\n",
    "    quant_best = successful_df[successful_df['algorithm'] == 'QuantAALTD2024']['accuracy'].max() if 'QuantAALTD2024' in successful_df['algorithm'].values else 0\n",
    "    hydra_best = successful_df[successful_df['algorithm'] == 'HydraAALTD2024']['accuracy'].max() if 'HydraAALTD2024' in successful_df['algorithm'].values else 0\n",
    "    old_best = successful_df[successful_df['algorithm'] == 'OldEnsemble']['accuracy'].max() if 'OldEnsemble' in successful_df['algorithm'].values else 0\n",
    "    new_best = successful_df[successful_df['algorithm'] == 'NewEnsemble']['accuracy'].max() if 'NewEnsemble' in successful_df['algorithm'].values else 0\n",
    "    \n",
    "    individual_best = max(quant_best, hydra_best)\n",
    "    \n",
    "    print(f\"\\n📈 Ensemble Effectiveness:\")\n",
    "    print(f\"   Best individual algorithm: {individual_best:.4f}\")\n",
    "    print(f\"   Old ensemble (data leak):  {old_best:.4f} ({old_best-individual_best:+.4f})\")\n",
    "    print(f\"   New ensemble (clean CV):   {new_best:.4f} ({new_best-individual_best:+.4f})\")\n",
    "    \n",
    "    if new_best > old_best:\n",
    "        print(f\"   ✅ Clean ensemble outperforms old ensemble by {new_best-old_best:.4f}\")\n",
    "    elif old_best > new_best:\n",
    "        print(f\"   ⚠️  Old ensemble still ahead by {old_best-new_best:.4f} (but has data leakage)\")\n",
    "    else:\n",
    "        print(f\"   🔄 Both ensembles perform similarly\")\n",
    "    \n",
    "    # Runtime analysis\n",
    "    print(f\"\\n⏱️  Runtime Analysis:\")\n",
    "    runtime_summary = successful_df.groupby('algorithm')['total_time'].agg(['mean', 'std']).round(2)\n",
    "    for algo, row in runtime_summary.iterrows():\n",
    "        print(f\"   {algo:20s}: {row['mean']:.1f} ± {row['std']:.1f} seconds\")\n",
    "    \n",
    "    # Configuration insights\n",
    "    print(f\"\\n⚙️  Configuration Insights:\")\n",
    "    config_perf = successful_df.groupby('config')['accuracy'].agg(['mean', 'std']).round(4)\n",
    "    for config, row in config_perf.iterrows():\n",
    "        print(f\"   {config:10s}: {row['mean']:.4f} ± {row['std']:.4f} average accuracy\")\n",
    "\n",
    "print(\"\\n✅ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Results saved to: /Users/urav/code/research/experiments/results/ensemble_comparison_Pedestrian_20250922_125426.csv\n",
      "⚙️  Configuration saved to: /Users/urav/code/research/experiments/results/config_Pedestrian_20250922_125426.txt\n",
      "\n",
      "🎉 Ensemble comparison complete!\n"
     ]
    }
   ],
   "source": [
    "# Export results for further analysis\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "results_dir = \"/Users/urav/code/research/experiments/results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save results with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = f\"{results_dir}/ensemble_comparison_{DATASET_NAME}_{timestamp}.csv\"\n",
    "\n",
    "df.to_csv(results_file, index=False)\n",
    "print(f\"📁 Results saved to: {results_file}\")\n",
    "\n",
    "# Also save configuration for reproducibility\n",
    "config_file = f\"{results_dir}/config_{DATASET_NAME}_{timestamp}.txt\"\n",
    "with open(config_file, 'w') as f:\n",
    "    f.write(f\"Dataset: {DATASET_NAME}\\n\")\n",
    "    f.write(f\"Train PCT: {TRAIN_PCT}\\n\")\n",
    "    f.write(f\"Test PCT: {TEST_PCT}\\n\")\n",
    "    f.write(f\"Random Seed: {RANDOM_SEED}\\n\")\n",
    "    f.write(f\"Configurations: {CONFIGS}\\n\")\n",
    "\n",
    "print(f\"⚙️  Configuration saved to: {config_file}\")\n",
    "print(\"\\n🎉 Ensemble comparison complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
