{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Debug: TSC Algorithms on Pedestrian Dataset\n",
    "\n",
    "Simple sanity check for all tsckit algorithms:\n",
    "- **Dataset**: Pedestrian (1% train, 1% test for speed)\n",
    "- **Algorithms**: All 10 algorithms including both ensembles\n",
    "- **Purpose**: Verify everything works after code changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.extend([\n",
    "    '/Users/urav/code/research',                    # For tsckit package\n",
    "    '/Users/urav/code/research/quant/code',         # For original quant.py\n",
    "    '/Users/urav/code/research/hydra/code',         # For original hydra.py  \n",
    "    '/Users/urav/code/research/aaltd2024/code',     # For quant_aaltd.py, hydra_gpu.py, utils.py, ridge.py\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsckit import Experiment, MonsterDataset\n",
    "\n",
    "from tsckit.algorithms import (\n",
    "    AeonAlgorithm, HydraAALTD2024, HydraOriginal, QuantAALTD2024, QuantOriginal,\n",
    "    HydraQuantStackedAALTD2024, # Old ensemble (data leakage)\n",
    "    HydraQuantStacked           # New clean ensemble (proper CV)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and experiment\n",
    "dataset = MonsterDataset(\"FordChallenge\", fold=0, train_pct=1, test_pct=1)\n",
    "print(\"Dataset info:\")\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all algorithms to experiment\n",
    "algorithms = [\n",
    "    # Original implementations\n",
    "    QuantOriginal(depth=6),\n",
    "    HydraOriginal(k=4, g=16, seed=42),\n",
    "    \n",
    "    # AALTD2024 implementations\n",
    "    QuantAALTD2024(num_estimators=50),  # Reduced for speed\n",
    "    HydraAALTD2024(k=4, g=16, seed=42),\n",
    "    \n",
    "    # AEON implementations\n",
    "    AeonAlgorithm(algorithm=\"quant\"),\n",
    "    AeonAlgorithm(algorithm=\"hydra\", n_kernels=4, n_groups=16),\n",
    "    AeonAlgorithm(algorithm=\"rocket\", n_kernels=500),\n",
    "    AeonAlgorithm(algorithm=\"multirocket\", n_kernels=100, max_dilations_per_kernel=16),\n",
    "    \n",
    "    # Ensembles\n",
    "    HydraQuantStackedAALTD2024(hydra_k=4, hydra_g=16, hydra_seed=42, quant_estimators=50),  # Old (data leakage)\n",
    "    HydraQuantStacked(n_folds=3, hydra_k=4, hydra_g=16, hydra_seed=42, n_estimators=50),    # New (clean CV)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment with all algorithms\n",
    "exp = Experiment(\n",
    "    name=\"debug_pedestrian\",\n",
    "    datasets=[dataset],\n",
    "    algorithms=algorithms\n",
    ")\n",
    "\n",
    "print(f\"üî¨ Experiment setup complete: {len(algorithms)} algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments\n",
    "exp.run(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary\n",
    "print(exp.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed results analysis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = exp.results_df()\n",
    "successful = df[df['status'] == 'success'].copy()\n",
    "\n",
    "if len(successful) > 0:\n",
    "    # Sort by accuracy for better visualization\n",
    "    successful = successful.sort_values('accuracy', ascending=True)\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    ax1.barh(range(len(successful)), successful['accuracy'])\n",
    "    ax1.set_yticks(range(len(successful)))\n",
    "    ax1.set_yticklabels(successful['algorithm_name'], fontsize=10)\n",
    "    ax1.set_xlabel('Accuracy')\n",
    "    ax1.set_title('Algorithm Accuracy Comparison')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add accuracy values as text\n",
    "    for i, v in enumerate(successful['accuracy']):\n",
    "        ax1.text(v + 0.005, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    # Runtime comparison\n",
    "    ax2.barh(range(len(successful)), successful['total_time'])\n",
    "    ax2.set_yticks(range(len(successful)))\n",
    "    ax2.set_yticklabels(successful['algorithm_name'], fontsize=10)\n",
    "    ax2.set_xlabel('Total Time (seconds)')\n",
    "    ax2.set_title('Algorithm Runtime Comparison')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add time values as text\n",
    "    for i, v in enumerate(successful['total_time']):\n",
    "        ax2.text(v + 0.01, i, f'{v:.2f}s', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Ensemble comparison table\n",
    "    print(\"\\nüìä Key Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Find ensemble results\n",
    "    old_ensemble = successful[successful['algorithm_name'].str.contains('HydraQuantStacked\\\\(hydra_k')]\n",
    "    new_ensemble = successful[successful['algorithm_name'].str.contains('HydraQuantStacked\\\\(folds')]\n",
    "    \n",
    "    if len(old_ensemble) > 0 and len(new_ensemble) > 0:\n",
    "        print(f\"üîÑ Old Ensemble (data leakage):   {old_ensemble.iloc[0]['accuracy']:.4f}\")\n",
    "        print(f\"‚ú® New Ensemble (clean CV):      {new_ensemble.iloc[0]['accuracy']:.4f}\")\n",
    "        improvement = new_ensemble.iloc[0]['accuracy'] - old_ensemble.iloc[0]['accuracy']\n",
    "        print(f\"üìà Improvement: {improvement:+.4f}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Algorithm: {successful.iloc[-1]['algorithm_name']} ({successful.iloc[-1]['accuracy']:.4f})\")\n",
    "    print(f\"‚ö° Fastest Algorithm: {successful.loc[successful['total_time'].idxmin(), 'algorithm_name']} ({successful['total_time'].min():.3f}s)\")\n",
    "else:\n",
    "    print(\"‚ùå No successful runs to analyze\")\n",
    "\n",
    "# Show any failures\n",
    "failed = df[df['status'] == 'failed']\n",
    "if len(failed) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  {len(failed)} algorithms failed:\")\n",
    "    for _, row in failed.iterrows():\n",
    "        print(f\"   - {row['algorithm_name']}: {row['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Debug completed succesxwsfully!\")\n",
    "print(\"üí° Note: Results not saved (debug mode). Use output_dir for persistent storage.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
