%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}        % Style for submissions to Nature Portfolio journals
\documentclass[pdflatex,sn-basic]{sn-jnl}           % Basic Springer Nature Reference Style/Chemistry Reference Style
%%\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}  % Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}   % Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}           % American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl} % Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}  % Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}           % APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}       % Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}               %  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]    % meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}     % to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Article Title}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}

\author[2,3]{\fnm{Second} \sur{Author}}\email{iiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

\affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

\affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensemble methods like HIVE-COTE 2.0 achieve state-of-the-art accuracy, they require 340 hours training time, making them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. We combine Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, evaluating performance on 10 large-scale MONSTER datasets with training sets ranging from 7,898 to 1,168,774 instances. Our strongest configuration achieves 0.836 mean accuracy compared to 0.829 for the best base algorithm (0.72\% improvement), improving on 7 of 10 datasets. However, ensembles capture only 11\% of theoretical oracle potential on average, revealing a substantial meta-learning optimization gap. Contrary to expectations, feature-level complementarity shows no significant correlation with ensemble gains ($r=0.328$, $p=0.36$), while prediction-level complementarity (oracle potential) shows moderate correlation ($r=0.631$, $p=0.050$). The central finding is that the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists, indicating that improved combination strategies could potentially double or triple ensemble gains.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{keyword1, Keyword2, Keyword3, Keyword4}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1}

Time series classification confronts a fundamental tension between accuracy and computational efficiency. Modern applications generate time series data at unprecedented scales: wearable sensors producing millions of activity sequences, satellite systems capturing continuous Earth observations, and industrial monitoring generating real-time sensor streams. Classification algorithms must process this data with both high accuracy and practical efficiency, yet current approaches struggle to achieve both simultaneously.

State of the art accuracy in time series classification comes from comprehensive ensemble methods like HIVE-COTE 2.0, which systematically combines algorithms from multiple representational paradigms \citep{hive-cote-2}. This multi-paradigm approach captures the principle that different problem types favor different algorithmic strategies \citep{tsc-bakeoff}. However, achieving this accuracy requires prohibitive computational cost: HIVE-COTE 2.0 requires 340 hours to train on standard benchmarks compared to 2.85 hours for efficient single-algorithm approaches like ROCKET \citep{hive-cote-2}. This 120-fold computational overhead renders comprehensive ensembles impractical for large-scale real-world datasets, where training times would extend to weeks or months.

Recent algorithmic advances demonstrate that efficiency and accuracy need not be entirely opposed. Hydra \citep{hydra}, using competing convolutional kernels to identify dominant local patterns, and Quant \citep{quant}, extracting quantile features from hierarchical intervals, both achieve competitive accuracy while training in under one hour on benchmark datasets. These algorithms represent distinct paradigms: Hydra captures pattern frequencies through kernel competition while Quant summarizes distributional statistics, suggesting potential complementarity. However, almost all prior work on ensemble methods for time series classification has focused on the UCR archive, a collection of relatively small benchmark datasets. Whether ensemble strategies remain effective on larger, real-world datasets from repositories like MONSTER \citep{monster} remains essentially unknown.

This work investigates a fundamental question: can targeted combination of two efficient algorithms from complementary paradigms capture the accuracy benefits of comprehensive ensembles while maintaining computational feasibility for large-scale problems? We systematically explore ensemble strategies for combining Hydra and Quant across six distinct configurations, ranging from simple feature concatenation to sophisticated stacked generalization with out-of-fold prediction generation. Rather than assuming complementarity, we empirically validate it through comprehensive analysis measuring both feature-level correlation and prediction-level error independence.

Our investigation proceeds in three stages. First, we quantify complementarity between Hydra and Quant across 10 large-scale MONSTER datasets, measuring feature space correlation, error pattern independence, and oracle ensemble potential (the theoretical accuracy ceiling achievable by perfect combination). Second, we systematically evaluate six ensemble configurations representing the major combination strategies: feature concatenation with linear and non-linear classifiers, asymmetric stacking that augments Quant features with Hydra predictions, symmetric stacking that combines out-of-fold predictions from both algorithms, and weighted averaging through a simplified Cross-validation Accuracy Weighted Probabilistic Ensemble (CAWPE). Third, we analyze what determines ensemble success, correlating complementarity metrics with actual performance gains.

Our findings reveal both promise and limitations. Ensembles achieve measurable but modest improvements over individual base algorithms, improving performance on the majority of datasets tested. However, actual gains fall far short of theoretical potential, with current meta-learning strategies capturing only a small fraction of the oracle upper bound. Analysis reveals that prediction-level complementarity (whether algorithms make mistakes on different samples) correlates with ensemble success, while feature-level complementarity (whether algorithms use different representations) does not. This suggests the primary challenge lies not in ensuring algorithms are different, but in developing meta-learners that can effectively exploit the complementarity that exists.

The remainder of this paper is organized as follows. Section 2 reviews related work on ensemble methods, efficient TSC algorithms, and theoretical foundations. Section 3 describes our methodology for complementarity analysis and ensemble design. Section 4 details the experimental setup including datasets, implementation, and evaluation protocols. Section 5 presents results from complementarity analysis, ensemble performance comparison, and predictor analysis. Section 6 discusses implications for ensemble design and identifies limitations. Section 7 concludes with practical recommendations and future directions.

\clearpage

\section{Related Work}\label{sec2}

Time series classification confronts a fundamental challenge: different problem types favor different algorithmic approaches. While combining diverse algorithms achieves state of the art accuracy on standard benchmark datasets, computational costs often exceed 300 hours of training. Recent fast algorithms reduce this to under 2 hours but sacrifice the benefits of combination. Moreover, almost all work on ensembles for time series classification has been conducted on the UCR archive, leaving the behavior and effectiveness of ensemble strategies on larger, real-world datasets essentially unknown. This work explores whether targeted ensemble strategies can capture complementarity while maintaining efficiency on larger-scale problems.

\subsection{Paradigm Diversity in Time Series Classification}

The comprehensive evaluation by \citet{tsc-bakeoff} established that time series classification problems exhibit substantial heterogeneity, identifying five primary algorithmic paradigms \citep[Section~2, pp.~610--611]{tsc-bakeoff}: whole series methods using elastic distance measures like dynamic time warping \citep[p.~611]{tsc-bakeoff}, interval based methods extracting statistical features from subseries, shapelet based approaches discovering phase independent discriminative subsequences, dictionary methods classifying based on recurring symbolic patterns, and combination approaches integrating multiple paradigms.

The critical empirical finding is that no single paradigm dominates across all problem types \citep[Table~11, p.~648]{tsc-bakeoff}. Performance differences between optimal and suboptimal paradigms range from 5\% to 15\%, with gaps exceeding 14\% when dictionary methods are optimal compared to whole series approaches \citep[Table~11, p.~648]{tsc-bakeoff}. This paradigm diversity establishes the theoretical motivation for ensemble methods: combining approaches from different paradigms can capture discriminatory features that no single method detects.

\subsection{Ensemble Approaches}

\subsubsection{HIVE-COTE: Comprehensive Multi-Paradigm Combination}

HIVE-COTE 2.0 demonstrates that systematic multi-paradigm combination achieves state of the art performance by integrating four component classifiers from distinct representational domains \citep[p.~3]{hive-cote-2}: the Shapelet Transform Classifier for phase independent patterns, the Temporal Dictionary Ensemble for frequency features, the Diverse Representation Canonical Interval Forest for interval statistics, and Arsenal for convolutional features.

The combination strategy employs Cross-validation Accuracy Weighted Probabilistic Ensemble (CAWPE) weighting \citep{cawpe}, raising each component's estimated accuracy to power $\alpha=4$ to magnify competence differences \citep[p.~1675]{cawpe}. This approach improved time series classification accuracy from 85.97\% to 87.16\% on the UCR classification benchmark \citep[Figure~8, p.~1692]{cawpe}, a 1.19 percentage point gain through combination methodology alone.

HIVE-COTE 2.0 ranks first on 112 UCR datasets \citep[p.~3]{hive-cote-2} but requires 340.21 hours training compared to 2.85 hours for ROCKET \citep[Table~4, p.~18]{hive-cote-2}, a 119-fold increase, while ROCKET achieves only marginally lower accuracy (ranking 5th versus HC2's 1st). This computational expense, combined with the practical impossibility of applying HC2 to larger real-world datasets, motivates development of efficient algorithms that capture complementarity without prohibitive cost.

\subsubsection{Theoretical Foundations}

Ensemble theory establishes that combining diverse algorithms can exceed individual performance when base learners capture complementary discriminatory features. Stacked generalization \citep{stacked-generalization} provides the meta-learning framework: ensembles work by deducing the biases of base algorithms with respect to the learning set \citep[p.~241]{stacked-generalization}, training a meta-learner to identify when each algorithm produces reliable predictions. The framework partitions training data to generate out of fold predictions, preventing meta-learners from memorizing training errors \citep[p.~244]{stacked-generalization}.

Effective combination requires base algorithms that span the space of generalizers and are mutually orthogonal \citep[p.~256]{stacked-generalization}, capturing different problem aspects and making independent errors. The paradigm diversity demonstrated by \citet[Table~11, p.~648]{tsc-bakeoff} provides empirical support: different algorithmic families excel on different problem types, creating opportunity for complementary combination. CAWPE's exponential weighting exploits this by allowing stronger performers on specific problems to dominate while preserving diversity \citep[p.~1675]{cawpe}, balancing trust in accuracy estimates against hedging through combination.

\subsection{Fast Algorithms for Time Series Classification}

\subsubsection{ROCKET: Random Convolutional Kernels}

ROCKET \citep{rocket} demonstrated that random convolutional kernels combined with simple linear classifiers achieve state of the art accuracy at a fraction of traditional computational expense. The method generates 10,000 random kernels with varying parameters and applies two pooling operators: proportion of positive values (PPV), capturing the fraction of the series matching kernel patterns, and global max pooling \citep[Section~3, pp.~1462--1463]{rocket}. While individual random kernels provide approximate feature detection, the ensemble of diverse kernels collectively captures discriminatory patterns \citep[p.~1455]{rocket}. ROCKET trained in under 2 hours for 85 datasets compared to over 11 days for HIVE-COTE \citep[p.~1455]{rocket}, establishing random convolution as a viable paradigm.

\subsubsection{Hydra: Competing Kernels}

Building on ROCKET's foundation, Hydra introduces a competing kernel mechanism bridging convolutional and dictionary approaches \citep{hydra}. The method organizes kernels into $g=64$ groups of $k=8$ kernels each (default configuration from the original implementation) \citep[Section~3.2, p.~1788]{hydra}. At each time point, only the kernel with largest magnitude response within each group contributes to the feature representation. The algorithm counts how often each kernel wins through hard counting (binary indicator) or soft counting (accumulated magnitude) \citep[Section~3.2, pp.~1789--1790]{hydra}.

This per time point competition connects to dictionary methods: just as dictionary approaches count word frequency, Hydra counts dominant kernel pattern frequency \citep[p.~1786]{hydra}. By varying group size $g$ and kernels per group $k$, the method interpolates between ROCKET's global pooling and dictionary style competition \citep[Figure~2, p.~1791]{hydra}. Hydra preserves temporal structure that global pooling discards, achieving accuracy comparable to dictionary methods while training in 31 minutes on 106 datasets \citep[p.~1781]{hydra}. Combining Hydra with MultiROCKET produces ensembles competitive with HIVE-COTE \citep[pp.~1796--1797]{hydra}.

\subsubsection{Quant: Interval Quantiles}

Quant represents a minimalist interval method using quantile features from fixed dyadic intervals \citep{quant}. The method recursively divides the series at depth $d=6$ (default configuration), creating hierarchical decompositions at scales $\{2^0, 2^1, \ldots, 2^5\}$ \citep[p.~2384]{quant}. For each interval of length $m$, Quant computes $k = 1 + \lfloor (m-1)/4 \rfloor$ evenly spaced quantiles, subtracting the interval mean from every second quantile to encode both absolute distributions and shift invariant shape at no additional cost \citep[p.~2385, Figure~4, p.~2386]{quant}. Features are extracted from four representations: original series, first order differences, second order differences, and Fourier transform magnitude \citep[Section~3.1, p.~2384]{quant}.

Quantiles subsume traditional summary statistics \citep[p.~2378]{quant} while enabling the classifier to handle interval selection through 10\% feature sampling at each split \citep[Section~4.2.6, p.~2387]{quant}. Quant trains in under 15 minutes on 142 datasets \citep[p.~2390]{quant}, ranking sixth in the 2024 comprehensive comparison \citep[Table~14, p.~2008]{bakeoff-redux}.

\subsection{Strategies for Combining Algorithms}

Three primary approaches exist for combining base algorithms into ensembles. \textbf{Feature concatenation} merges feature matrices $\mathbf{H} \in \mathbb{R}^{n \times d_H}$ and $\mathbf{Q} \in \mathbb{R}^{n \times d_Q}$ into $[\mathbf{H} \mid \mathbf{Q}]$ for a single classifier. This allows learning interactions between heterogeneous features but creates high dimensional spaces requiring classifiers with appropriate feature sampling strategies.

\textbf{Stacked generalization} \citep{stacked-generalization} operates on algorithm predictions rather than raw features. Training must use out of fold predictions to prevent overfitting \citep[p.~244]{stacked-generalization}: partition data into $K$ folds, train base algorithms on $K-1$ folds, generate predictions for the held out fold, then train the meta-learner on these out of fold predictions. This ensures the meta-learner learns generalizable combination patterns rather than memorizing training errors. The framework accommodates asymmetric designs where deterministic transformations (Quant features) combine with fitted model predictions (Hydra logits).

\textbf{Weighted averaging} through CAWPE requires no meta-learner training \citep{cawpe}. Each algorithm's cross-validation accuracy determines weight $w_j$, raised to power $\alpha=4$ to amplify competence differences \citep[p.~1675]{cawpe}:
\begin{equation}
P(y=i \mid \mathbf{x}) = \frac{\sum_j w_j^\alpha \cdot P_j(y=i \mid \mathbf{x})}{\sum_j w_j^\alpha}
\end{equation}
This balances trusting accuracy estimates against hedging through combination, performing well on small training sets \citep[Table~3, p.~1693]{cawpe}.

\subsection{Research Gap}

The preceding sections establish that paradigm diversity motivates ensembles (Section 2.1), comprehensive ensembles achieve state of the art accuracy but require 340 hours (Section 2.2), efficient algorithms like Hydra and Quant train in under 1 hour (Section 2.3), and theoretical frameworks exist for combination (Section 2.4). What remains unexplored is whether targeted combination of two efficient algorithms from complementary paradigms captures ensemble benefits while maintaining efficiency.

Hydra (competing convolutional kernels identifying dominant local patterns) and Quant (quantile based distributional statistics from hierarchical intervals) represent distinct paradigms mirroring HIVE-COTE's multi-paradigm coverage through two efficient components. Evidence that combining efficient methods shows promise exists: MultiROCKET combined with Hydra achieves 0.884 accuracy, ranking second and approaching HIVE-COTE \citep[Table~14, p.~2008]{bakeoff-redux}. However, that combination employs two convolution methods from the same paradigm. Whether cross-paradigm combination of Hydra and Quant yields similar benefits, and which combination strategy (concatenation, stacking, or CAWPE) proves most effective, remains an empirical question this work addresses.

\clearpage

\section{Methodology}\label{sec3}

We develop systematic ensembles combining Hydra and Quant through comprehensive exploration of the ensemble design space. Prior work established that TSC problems benefit from combining classifiers built on different representations \citep[p.~3213]{hive-cote-2}, as different paradigms capture discriminatory features in distinct data domains. We validate this principle for Hydra and Quant through complementarity analysis before constructing ensembles, providing empirical foundation for the combination strategies.

\subsection{Complementarity Analysis Framework}\label{subsec:complementarity}

To validate that Hydra and Quant capture orthogonal discriminatory information, we measure complementarity at both feature level (representation similarity) and prediction level (error independence). These metrics quantify the diversity that ensemble theory identifies as necessary for performance gains \citep{stacked-generalization}.

\textbf{Feature-level complementarity.} We compute the Pearson correlation between each Hydra feature and each Quant feature, forming a cross-correlation matrix. For computational tractability on datasets with many features, we subsample up to 5,000 test instances. The median of the maximum absolute correlations (for each Hydra feature, its maximum absolute correlation with any Quant feature) quantifies feature space overlap. Low median correlation indicates that Hydra and Quant capture distinct aspects of the time series. We supplement this with canonical correlation analysis (CCA) to identify the dimensionality and strength of shared linear subspaces between the feature representations.

\textbf{Prediction-level complementarity.} We define error vectors $\mathbf{e}_H$ and $\mathbf{e}_Q$ where $e_i = 1$ if algorithm misclassifies sample $i$, and 0 otherwise. The Pearson correlation $\rho(\mathbf{e}_H, \mathbf{e}_Q)$ quantifies error pattern similarity. Low error correlation indicates algorithms make mistakes on different samples, a favorable condition for ensemble combination. We also measure disagreement rate (fraction of samples where $\hat{y}_H \neq \hat{y}_Q$) and construct the oracle ensemble: a hypothetical perfect combiner that selects the correct prediction whenever at least one algorithm is correct.

The oracle ensemble accuracy provides an upper bound \textit{specifically for prediction-combination strategies} (voting, weighting, meta-learning from logits). This bound does not constrain feature-concatenation ensembles: a joint model trained on concatenated Hydra and Quant features can potentially exceed the oracle bound by learning feature interactions invisible to individual models. The oracle gain, defined as oracle accuracy minus the better individual algorithm's accuracy, quantifies the theoretical improvement available through prediction-combination strategies.

\subsection{Ensemble Design Space}\label{subsec:design_space}

Combining Hydra and Quant requires several design decisions: what representations to combine (raw features, predictions, or logits), where to apply the meta-learner, and which meta-learner architecture to use. We systematically explore this design space through six ensemble configurations that cover the primary combination strategies while maintaining computational feasibility.

\subsubsection{Feature Concatenation Ensembles}

The simplest combination strategy concatenates Hydra and Quant features and trains a single classifier on the joint representation: $[\mathbf{H} \mid \mathbf{Q}] \rightarrow \text{Classifier}$, where $\mathbf{H} \in \mathbb{R}^{n \times d_H}$ represents Hydra features ($d_H \approx 10{,}000$) and $\mathbf{Q} \in \mathbb{R}^{n \times d_Q}$ represents Quant features ($d_Q$ proportional to series length).

This approach requires careful consideration of the meta-learner. Hydra's features are individually weak: the competing kernel paradigm means each feature simply counts pattern occurrences, with discriminative power emerging from combinations of many features. Quant's interval quantiles are similarly distributional summaries rather than strong individual discriminators. A linear classifier like Ridge may struggle to exploit these heterogeneous feature spaces effectively, as it cannot learn the non-linear interactions between pattern counts and distributional statistics.

We therefore test two classifiers for the joint feature space. \textbf{Ridge (FC-Ridge)} provides a linear baseline with L2 regularization (Ridge is Hydra's default classifier), offering computational efficiency and built-in regularization for the high-dimensional feature space ($d_H + d_Q \approx 10{,}000+$). \textbf{ExtraTrees (FC-ET)} uses an ensemble of 200 extremely randomized decision trees with entropy criterion and max features set to 0.1 (ExtraTrees is Quant's default classifier; a random subset of 10\% of all features are considered at each split). This non-linear ensemble can learn complex decision boundaries and feature interactions. The linear proportion of features per split (rather than $\sqrt{p}$) is critical: with large feature spaces, a sublinear number of candidate features per split could result in the classifier running out of training samples before adequately exploring the feature space \citep{quant}.

Feature concatenation has a key advantage: both Hydra and Quant features can be computed directly on the training set without information leakage concerns, as neither involves fitted model predictions at the feature extraction stage.

\subsubsection{Asymmetric Stacking: Quant Features with Hydra Logits}

Stacking \citep{stacked-generalization} provides an alternative to raw feature concatenation by using one algorithm's predictions as inputs to another. For Hydra and Quant, an asymmetric stacking approach combines Quant's features (deterministic transformations) with Hydra's logits (outputs of a fitted Ridge classifier): $[\mathbf{Q} \mid \mathbf{L}_H] \rightarrow \text{Classifier}$, where $\mathbf{L}_H \in \mathbb{R}^{n \times c}$ represents Hydra's class probability estimates ($c$ is number of classes). This arrangement provides a simple method to aggregate information from Hydra features (which are individually weak pattern counts) and feed these aggregated predictions in conjunction with Quant's distributional features to the final classifier.

This design reflects the distinction between fitted and non-fitted representations. Quant features are interval quantiles: deterministic transformations that do not depend on training data labels. Computing these on the full training set introduces no information leakage. However, Hydra's logits come from a Ridge classifier trained on Hydra features. Using predictions from a model trained on the same data the meta-learner will train on violates proper train-test separation and leads to overoptimistic performance estimates. This overfitting degrades generalization performance: the meta-learner learns to trust patterns that exist due to memorization rather than true predictive signal.

We address this through out-of-fold (OOF) prediction generation. The training data $\mathcal{D}_{\text{train}}$ is partitioned into $K=5$ stratified folds $\{F_1, ..., F_5\}$. For each fold $F_k$: (1) train Hydra on the remaining $K-1$ folds: $\mathcal{D}_{\text{train}} \setminus F_k$, and (2) generate logits $\mathbf{L}_H^{(k)}$ for samples in $F_k$. After iterating through all folds, we obtain $\mathbf{L}_H^{\text{OOF}} \in \mathbb{R}^{n \times c}$ for the entire training set. Crucially, for any sample $i \in F_k$, the logits $\mathbf{L}_H^{\text{OOF}}[i]$ were generated by a Hydra model that never encountered sample $i$ during training.

The meta-level training data is $\mathbf{X}_{\text{meta}} = [\mathbf{Q} \mid \mathbf{L}_H^{\text{OOF}}]$. For test data, we train Hydra on the complete training set to generate test logits $\mathbf{L}_H^{\text{test}}$, extract Quant features $\mathbf{Q}^{\text{test}}$, and produce final predictions from $[\mathbf{Q}^{\text{test}} \mid \mathbf{L}_H^{\text{test}}]$ via the trained meta-learner.

This asymmetric design offers several advantages. The meta-learner has access to both raw distributional features (Quant) and aggregated pattern information (Hydra logits), providing complementary views. Hydra logits encode confidence in class predictions through the Ridge classifier's learned weights, while Quant features capture raw statistical information about value distributions. The meta-learner can learn context-dependent combination strategies: when to trust Hydra's pattern-based classifications versus when to rely on Quant's distributional evidence.

We test two meta-learners: \textbf{Ridge (QFeat-HLogit-Ridge)} for linear combination, and \textbf{ExtraTrees (QFeat-HLogit-ET)} for non-linear combination enabling complex decision rules.

\subsubsection{Symmetric Stacking: Dual Out-of-Fold Predictions}

A symmetric alternative generates OOF predictions for both algorithms and trains the meta-learner on these predictions alone: $[\mathbf{L}_H^{\text{OOF}} \mid \mathbf{L}_Q^{\text{OOF}}] \rightarrow \text{ExtraTrees}$. This approach treats both algorithms identically, with neither receiving preferential treatment through raw feature access.

For Quant, we perform the same $K$-fold procedure: train Quant (feature extraction followed by ExtraTrees classifier) on $K-1$ folds, generate logits for the held-out fold, and iterate to produce $\mathbf{L}_Q^{\text{OOF}} \in \mathbb{R}^{n \times c}$. The meta-learner trains on class probabilities alone (2$c$-dimensional input), learning which algorithm to trust under which conditions based solely on their predicted class distributions.

We refer to this as \textbf{Dual-OOF-ET}. This symmetric design has theoretical appeal: it places both algorithms on equal footing and lets the meta-learner discover reliability patterns without bias toward one algorithm's feature space. However, it discards potentially useful information: the raw features that were used to generate the predictions. Whether this simplification aids or hinders meta-learning is an empirical question.

\subsubsection{Weighted Averaging: CAWPE}

Cross-validation Accuracy Weighted Probabilistic Ensemble (CAWPE) \citep{cawpe} provides an alternative to stacking that requires no meta-learner training. CAWPE estimates each algorithm's competence and uses these estimates to weight probability predictions.

For each algorithm $j$, we estimate accuracy $\text{acc}_j$ and compute weight $w_j = \text{acc}_j^\alpha$, where $\alpha=4$ exponentially amplifies differences in estimated competence. Final predictions combine weighted probabilities:
\begin{equation}
P(y=i \mid \mathbf{x}) = \frac{w_H \cdot P_H(y=i \mid \mathbf{x}) + w_Q \cdot P_Q(y=i \mid \mathbf{x})}{w_H + w_Q}
\end{equation}

This approach has two key advantages: computational simplicity (no additional classifier training) and theoretical grounding (the exponential weighting significantly outperformed simple averaging, stacking, and ensemble selection methods across 206 datasets \citep{cawpe}). The choice of $\alpha=4$ balances trusting accuracy estimates (higher $\alpha$ puts more weight on the seemingly better algorithm) against hedging through combination (lower $\alpha$ weights both algorithms more equally). Our implementation details are described in Section~\ref{subsec:implementation}.

\subsubsection{Summary of Ensemble Configurations}

Table~\ref{tab:ensemble_summary} summarizes our six ensemble configurations. The design space exploration covers feature concatenation (testing linear vs. non-linear meta-learners), asymmetric stacking with OOF predictions (again with both meta-learners), symmetric stacking treating both algorithms identically, and weighted averaging without meta-learning. This systematic approach enables us to identify which combination strategies effectively leverage Hydra-Quant complementarity while maintaining computational feasibility.

\begin{table}[h]
\centering
\caption{Summary of six ensemble configurations exploring the design space of combining Hydra and Quant. Specific hyperparameters for all configurations are detailed in Section~\ref{subsec:implementation}.}\label{tab:ensemble_summary}
\small
\begin{tabular}{llcc}
\toprule
\textbf{Strategy} & \textbf{Inputs to Meta-Learner} & \textbf{Meta-Learner} & \textbf{OOF?} \\
\midrule
FC-Ridge & $[\mathbf{H} \mid \mathbf{Q}]$ & Ridge & No \\
FC-ET & $[\mathbf{H} \mid \mathbf{Q}]$ & ExtraTrees & No \\
QFeat-HLogit-Ridge & $[\mathbf{Q} \mid \mathbf{L}_H^{\text{OOF}}]$ & Ridge & Yes (Hydra) \\
QFeat-HLogit-ET & $[\mathbf{Q} \mid \mathbf{L}_H^{\text{OOF}}]$ & ExtraTrees & Yes (Hydra) \\
Dual-OOF-ET & $[\mathbf{L}_H^{\text{OOF}} \mid \mathbf{L}_Q^{\text{OOF}}]$ & ExtraTrees & Yes (both) \\
CAWPE & Weighted prob. combination & None & No \\
\botrule
\end{tabular}
\end{table}

\clearpage

\section{Experimental Design}\label{sec4}

\subsection{Datasets}

We evaluate on 10 datasets from the MONSTER benchmark \citep{monster}, a large-scale time series classification archive designed to address computational and evaluation challenges of modern TSC. A key motivation for using MONSTER is that existing ensemble approaches like HIVE-COTE 2.0 are computationally constrained to smaller benchmarks like UCR. They cannot realistically be applied to larger, real-world datasets due to prohibitive training times. This work investigates whether efficient ensembles can capture complementarity benefits while remaining practically applicable to larger-scale problems. The 10 datasets were selected from an initial complementarity analysis based on successful completion of feature extraction and prediction generation for both algorithms (one dataset, S2Agri-10pc-34, was excluded due to GPU memory constraints on available compute nodes).

The selected datasets span diverse application domains and scales. Human activity recognition from wearable sensors includes UCIActivity (7,898 train / 2,401 test, 9 channels, 128 time points, 6 classes), USCActivity (43,060 / 13,168, 6 channels, 100 time points, 12 classes), WISDM (11,989 / 5,177, 3 channels, 100 time points, 6 classes), and WISDM2 (98,094 / 50,940, 3 channels, 100 time points, 6 classes). Vehicular and pedestrian sensor data includes FordChallenge (29,003 / 7,254, 30 channels, 40 time points, 2 classes), Traffic (1,168,774 / 292,194, 1 channel, 24 time points, 7 classes), and Pedestrian (151,696 / 37,925, 1 channel, 24 time points, 82 classes). Environmental monitoring includes LakeIce (103,424 / 25,856, 1 channel, 161 time points, 3 classes), remote sensing includes Tiselac (79,907 / 19,780, 10 channels, 23 time points, 9 classes), and audio classification includes InsectSound (40,000 / 10,000, 1 channel, 600 time points, 10 classes).

These 10 datasets provide coverage across univariate and multivariate problems, with training set sizes from 7,898 to 1,168,774 instances, class counts from 2 to 82, and both short (23 time points) and long (600 time points) series.

\subsection{Implementation}\label{subsec:implementation}

\textbf{Base Algorithms.} We employ Hydra and Quant as described in Section 2.3. For Hydra, we use $k=8$ kernels per group, $g=64$ groups (yielding 512 total kernels per dilation level), and random seed 42 for kernel initialization. For Quant, we use depth $d=6$ (creating dyadic intervals at scales $\{2^0, 2^1, ..., 2^5\}$) with divisor $v=4$ (computing $1 + \lfloor (m-1)/4 \rfloor$ quantiles per interval of length $m$). Quant operates on four representations: original series, smoothed first difference (moving average window=5), second difference, and discrete Fourier transform magnitude \citep{quant}.

\textbf{Feature Concatenation Ensembles.} Ridge uses L2 regularization with automatic alpha selection via cross-validation. ExtraTrees uses 200 estimators, entropy criterion, and max\_features=0.1 (10\% of features considered per split), following the Quant implementation \citep{quant}. This linear proportion of features prevents feature space exhaustion on high-dimensional problems.

\textbf{Stacking Ensembles.} For asymmetric and symmetric stacking, we employ 5-fold stratified cross-validation for out-of-fold prediction generation, following established TSC benchmarking protocols \citep{tsc-bakeoff}. The ExtraTrees meta-learners use the same configuration as feature concatenation (200 estimators, entropy criterion, max\_features=0.1). Ridge meta-learners use the same cross-validated alpha selection.

\textbf{CAWPE.} We use training set accuracy as a proxy for cross-validation accuracy estimation, with exponential weighting parameter $\alpha=4$. This is a simplification of the original CAWPE formulation, which employs proper cross-validation to estimate component accuracies \citep{cawpe}. We adopt in-sample accuracy estimation to reduce computational overhead for this systematic exploration of six ensemble configurations across 10 large-scale datasets. While this may overestimate algorithm performance compared to true cross-validation, the exponential weighting ($\alpha=4$) still magnifies competence differences as specified in the original method. This simplified approach serves as an initial investigation; future work should validate findings with proper cross-validation-based weighting.

\textbf{Computational Environment.} All experiments run on the M3 HPC cluster (Monash University) using Intel Xeon Gold 6548Y+ CPUs with 8 cores and 256GB RAM allocated per job. Hydra operations utilize GPU acceleration where available. We use Python 3.11 with PyTorch 2.0, scikit-learn 1.3, and NumPy 1.24.

All experiments use predefined train/test splits from the MONSTER benchmark to ensure reproducibility and fair comparison across all ensemble configurations. Detailed characteristics of all 10 datasets (training/test sizes, channels, sequence length, number of classes) are provided in Appendix~\ref{appB}.

\subsection{Evaluation Protocol}

We measure classification accuracy on held-out test sets, reporting results from single-run experiments (no repeated trials). For each dataset, we compute the ensemble gain as the difference between ensemble accuracy and the better-performing base algorithm: $\text{Gain} = \text{Acc}_{ensemble} - \max(\text{Acc}_{Hydra}, \text{Acc}_{Quant})$. Positive gains indicate the ensemble outperforms both base algorithms.

To contextualize ensemble performance relative to theoretical potential, we define oracle utilization as the ratio of actual ensemble gain to oracle gain:
\begin{equation}
\text{Oracle Utilization} = \frac{\text{Ensemble Gain}}{\text{Oracle Gain}} \times 100\%
\end{equation}
Negative oracle utilization indicates the ensemble performed worse than the better base algorithm, despite theoretical potential for improvement.

We quantify computational efficiency through wall-clock training time, measured from the start of feature extraction through final model fitting. For stacking ensembles, this includes cross-validation overhead for OOF prediction generation (training each base algorithm 5 times). For CAWPE, this includes model training for both algorithms. All timing measurements represent single runs on consistent hardware, reported as time per 1000 training samples to account for dataset size differences.

For complementarity analysis, we compute feature-level metrics (median cross-correlation between Hydra and Quant feature spaces, canonical correlation analysis) and prediction-level metrics (error correlation, disagreement rate, oracle ensemble accuracy) on test data after training both algorithms on the full training set. Feature extraction for complementarity analysis uses up to 5,000 randomly sampled test instances (random seed 42) to manage memory requirements for correlation computations on datasets with very large test sets. Importantly, the oracle utilization metric (Equation 1) applies specifically to prediction-combination strategies; feature-concatenation ensembles can exceed the oracle bound by learning novel feature interactions.

\clearpage

\section{Results}\label{sec5}

\subsection{Complementarity Analysis}

Following the complementarity analysis framework described in Section~\ref{subsec:complementarity}, we measured feature-level and prediction-level complementarity between Hydra and Quant across 10 MONSTER datasets (subsampled to 5,000 instances where necessary, per Section 4.3) to validate the hypothesis that these algorithms capture orthogonal discriminatory information. Figure~\ref{fig:complementarity} (page~\pageref{fig:complementarity}) visualizes the key complementarity patterns.

\textbf{Feature-level complementarity.} As detailed in Section~\ref{subsec:complementarity}, feature correlation (measured by median maximum absolute correlation between feature spaces and canonical correlation analysis) quantifies the extent to which Hydra and Quant capture overlapping versus orthogonal information. Median cross-correlation between Hydra and Quant feature representations ranged from 0.460 (FordChallenge) to 0.709 (USCActivity), with mean $0.583 \pm 0.089$. This moderate correlation indicates that while the feature spaces share some information, substantial complementarity exists. The lowest correlations occurred on high-dimensional multivariate data (FordChallenge: 0.460, Tiselac: 0.514), where Hydra's convolution-based patterns and Quant's quantile-based distributions capture fundamentally different signal characteristics. Higher correlations appeared on activity recognition data (USCActivity: 0.709, UCIActivity: 0.663), suggesting representational overlap when discriminatory information concentrates in common temporal features.

Canonical correlation analysis confirmed these patterns. The first canonical correlation averaged 0.997 across datasets (range: 0.987 to 1.000), indicating strong linear dependency in a single dimension, but this reflects the high dimensionality of both feature spaces rather than complete overlap. The shared information occupies low-dimensional subspaces while most feature dimensions remain independent.

\textbf{Prediction-level complementarity.} As noted in Section~\ref{subsec:complementarity}, prediction-level complementarity quantifies the extent to which different algorithms correctly classify given examples in complementary patterns (making errors on different samples rather than the same samples). Error correlation between Hydra and Quant predictions averaged $0.421 \pm 0.181$ across datasets, ranging from near-zero (UCIActivity: 0.139) to high correlation (Tiselac: 0.695). Low error correlation indicates complementary decision boundaries, where algorithms make mistakes on different samples. Disagreement rates averaged 21.9\%, with substantial variation (1.0\% on LakeIce to 39.8\% on Traffic), reflecting dataset-dependent complementarity strength.

Oracle ensemble accuracy, measuring the fraction of test samples correctly classified by at least one algorithm, exceeded the better individual algorithm by 0.0014 to 0.1239 (mean gain: 0.0471, or 4.71\% absolute improvement over best base). This quantifies the theoretical upper bound achievable through prediction-combination strategies. The largest gains appeared on multi-class problems with moderate individual accuracy (USCActivity: 12.39\% gain, InsectSound: 7.73\% gain, WISDM2: 6.14\% gain), where algorithms exhibit different class-specific strengths. Minimal gains occurred on problems where one algorithm dominates (LakeIce: 0.14\% gain, where Quant achieves 99.7\% accuracy).

\textbf{Feature concatenation exceeds oracle bound.} While the oracle bound constrains prediction-combination strategies, feature-concatenation ensembles can exceed it by learning interactions invisible to individual models. Empirically, FC-ExtraTrees correctly classified 12,556 test samples (across all 10 datasets) where both Hydra and Quant failed, demonstrating that joint feature-space models can indeed surpass the prediction-combination ceiling. This oracle-exceeding rate varied by dataset, from 3.6\% of "both wrong" samples (Tiselac) to 91.7\% (UCIActivity), indicating that the capacity to learn novel feature interactions depends on problem characteristics.

Figure~\ref{fig:complementarity} summarizes these findings. The results confirm that Hydra and Quant capture complementary discriminatory information, with complementarity strength varying across problem types. Unlike previous studies that inferred complementarity from performance differences across problem types, we directly quantified complementarity through feature correlation and error analysis within each dataset.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figure1_complementarity}
\caption{Complementarity between Hydra and Quant across 10 MONSTER datasets. (A) Algorithm performance complementarity showing accuracy of both algorithms on each dataset. Points away from the diagonal indicate performance differences. Univariate (blue) and multivariate (orange) datasets show no systematic pattern. (B) Oracle ensemble potential measuring theoretical upper bound for prediction-combination strategies. Green bars indicate high oracle potential (gain $>5\%$), orange indicates moderate potential ($2$--$5\%$), red indicates low potential ($<2\%$). Note that feature-concatenation ensembles can exceed these bounds.}\label{fig:complementarity}
\end{figure*}

\subsection{Ensemble Performance}

Table~\ref{tab:algorithm_comparison} presents comprehensive results comparing Hydra, Quant (two base algorithms), and the top three ensemble configurations across all 10 datasets (full results for all six ensemble configurations are provided in Appendix~\ref{appA}). As evident from the table, ensembles achieved the highest accuracy on 7 out of 10 datasets (outperforming both base methods), with QFeat-HLogit-ET and CAWPE proving most accurate overall rather than the symmetric DualOOF-ET approach. These three ensembles represent the strongest performers from our systematic design space exploration: QFeat-HLogit-ET (asymmetric stacking with ExtraTrees), CAWPE (weighted averaging), and DualOOF-ET (symmetric stacking).

\input{table1_algorithm_comparison}

\textbf{Ridge vs ExtraTrees meta-learners.} Figure~\ref{fig:ensemble}A shows the critical importance of meta-learner choice for feature concatenation ensembles. ExtraTrees consistently outperforms Ridge across 9 of 9 datasets (Traffic excluded due to missing Ridge configuration), with mean accuracy of 0.827 vs 0.797 for Ridge. This 3 percentage point advantage demonstrates that non-linear meta-learning is essential for exploiting heterogeneous feature spaces, at least on these larger datasets: Ridge cannot effectively combine Hydra's pattern-counting features with Quant's distributional statistics, while ExtraTrees learns complex decision boundaries that adaptively weight different feature types. This finding validates the design principle that individually weak features from competing kernels and interval quantiles require non-linear aggregation. The linear model (which works well with large feature spaces on smaller UCR datasets) may be underfitting on these larger problems, while ExtraTrees can learn more effectively in this context.

\textbf{Top ensemble performance.} Figure~\ref{fig:ensemble}B presents gains for the three best ensembles relative to the best base algorithm. QFeat-HLogit-ET achieved mean accuracy of 0.836, representing a 0.0072 absolute improvement (0.87\% relative gain) over the mean best-base accuracy of 0.829. The ensemble improved over the better base algorithm on 7 of 10 datasets (70\%), with largest improvements on USCActivity (+2.29pp), WISDM (+2.34pp), and InsectSound (+2.72pp). However, QFeat-HLogit-ET degraded performance on UCIActivity (--0.37pp) and Pedestrian (--0.02pp), showing that even the strongest ensemble configuration cannot universally improve over base algorithms.

CAWPE provided a different accuracy-consistency trade-off, achieving mean accuracy of 0.834 with more consistent but smaller gains. Its largest improvement was InsectSound (+1.10pp), with moderate gains on WISDM (+1.99pp) and USCActivity (+1.17pp). CAWPE degraded performance on 2 datasets (WISDM2: --0.54pp, Pedestrian: --1.97pp).

DualOOF-ET, despite its symmetric design, achieved mean accuracy of 0.831. Its strength lay in consistency: it provided the largest gain on UCIActivity (+8.33pp, where other ensembles struggled), but showed mixed results elsewhere.

\textbf{No hero ensemble.} Critically, no single ensemble dominates across all datasets. Of the 10 datasets, QFeat-HLogit-ET ranks first on 4, CAWPE on 3, and DualOOF-ET on 1 (with base algorithms winning the remaining 2). This finding demonstrates that ensemble effectiveness is problem-dependent: the optimal combination strategy depends on the specific complementarity pattern between algorithms for each dataset, rather than one strategy being universally superior.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figure2_ensemble_performance}
\caption{Ensemble performance analysis across 10 MONSTER datasets. (A) Classifier comparison for feature concatenation strategy: Ridge (red) vs ExtraTrees (green) meta-learners across 9 datasets. ExtraTrees consistently outperforms Ridge, demonstrating the importance of non-linear meta-learning for heterogeneous feature spaces. (B) Top three ensemble performance gains relative to best base algorithm. QFeat-HLogit-ET (primary ensemble), CAWPE (weighted averaging), and DualOOF-ET (symmetric stacking) show varying effectiveness across datasets, with largest improvements on USCActivity, WISDM, and InsectSound.}\label{fig:ensemble}
\end{figure*}

\subsection{Predictors of Ensemble Success}\label{subsec:predictors}

To understand what determines ensemble effectiveness, we analyzed correlations between complementarity metrics and ensemble gains, focusing on QFeat-HLogit-ET (our strongest configuration) as the primary analysis target. Table~\ref{tab:complementarity_summary} (Appendix~\ref{appC}) provides complete complementarity statistics for reference. Figure~\ref{fig:predictors} (page~\pageref{fig:predictors}) visualizes the relationship between complementarity metrics and ensemble gains.

\textbf{Oracle gain as key predictor.} We observed a moderate positive correlation between oracle gain and actual ensemble gain (Pearson $r=0.631$, $p=0.050$). While this correlation approaches statistical significance, the relationship shows considerable scatter, suggesting that oracle potential provides an upper bound estimate but does not strongly predict actual ensemble success. Other factors beyond oracle potential (such as meta-learner capacity and feature interactions) likely play important roles in determining ensemble effectiveness. The realized gains fall well below the theoretical ceiling: QFeat-HLogit-ET captures only 11.0\% of oracle potential on average (discussed below).

Despite the moderate strength and marginal significance of this correlation, the finding suggests that oracle potential sets an upper bound on achievable gains. When one algorithm dominates completely (as Quant does on LakeIce with 99.7\% accuracy, yielding only 0.14\% oracle gain), even highly complementary representations yield minimal ensemble improvements. Conversely, datasets with substantial oracle potential (such as USCActivity with 12.39\% oracle gain) provide favorable conditions for ensemble methods, though actual utilization remains low (discussed below).

This principle has practical consequences for algorithm selection in ensemble contexts. The results suggest we should seek pairs of algorithms that are both: (1) individually strong (to maintain high baseline performance), and (2) complementary in their error patterns (to create oracle potential). Feature-space diversity without prediction-level complementarity is insufficient.

\textbf{Feature complementarity shows no correlation.} Contrary to initial hypotheses, median feature cross-correlation showed no significant correlation with ensemble gain (Pearson $r=0.328$, $p=0.36$). While the correlation is positive, it does not reach statistical significance, revealing that complementary feature representations alone do not guarantee ensemble success.

Consider two contrasting cases. FordChallenge exhibits strong feature-level complementarity (median correlation 0.460), yet achieves near-zero ensemble gain (+0.52pp). This occurs because Quant already achieves 92.3\% accuracy, leaving little room for improvement despite highly complementary features. Conversely, USCActivity shows weaker complementarity (correlation 0.709), yet achieves large ensemble gain (+2.29pp) because both base algorithms perform moderately (Hydra: 66.5\%, Quant: 74.9\%), and oracle analysis reveals substantial potential (12.39\% oracle gain).

This pattern indicates that complementarity is necessary but not sufficient for ensemble success. Algorithms must capture different information (complementarity), but they must also both be reasonably accurate and leave room for improvement (oracle potential).

We examined correlations between dataset properties (number of classes, series length, number of channels, training set size) and ensemble gain. None showed significant correlations (all $|r| < 0.4$, all $p > 0.1$), suggesting ensemble effectiveness depends more on algorithm-specific factors (base accuracies, error patterns) than on intrinsic dataset properties.

\begin{figure*}[tb]
\centering
\includegraphics[width=\textwidth]{figure3_predictors}
\caption{Predictors of ensemble success for QFeat-HLogit-ET across 10 MONSTER datasets. (A) Oracle gain shows moderate positive correlation with actual ensemble gain (Pearson $r=0.631$, $p=0.050$), approaching but not reaching statistical significance. The relationship shows considerable scatter, indicating oracle potential provides an upper bound but does not strongly predict actual gains. (B) Feature complementarity (median cross-correlation) shows no significant correlation with ensemble gain ($r=0.328$, $p=0.36$), revealing that complementary feature representations alone do not guarantee ensemble success.}\label{fig:predictors}
\end{figure*}

\textbf{Oracle utilization reveals optimization gap.} QFeat-HLogit-ET captured only 11.0\% of theoretical oracle potential on average, with substantial variation across datasets (range: --26.5\% to 49.4\%). Even on datasets where the ensemble succeeded, utilization remained low. The best case, WISDM (49.4\% utilization), still captured less than half the available potential. The worst case, UCIActivity (--26.5\% utilization), indicates the ensemble actually underperformed the better base algorithm despite oracle potential existing. USCActivity, despite achieving large absolute gain (+2.29pp from 12.39\% oracle potential), utilized only 18.5\% of the theoretical ceiling.

This low utilization suggests current meta-learning strategies are far from optimal. The meta-learner appears unable to reliably identify when to trust each algorithm's predictions, sometimes overweighting incorrect predictions or failing to leverage situations where algorithms make complementary errors. Several factors may contribute. First, the meta-learner trains on a relatively small number of OOF predictions ($n$ training samples), which may be insufficient to learn reliable combination rules. Second, the 2$c$-dimensional logit space (for $c$ classes) may not contain sufficient information about when each algorithm should be trusted: the confidence scores alone may not reveal the problem characteristics that determine algorithm reliability. Third, ExtraTrees may lack the capacity to learn the complex conditional logic required for optimal algorithm selection.

\clearpage

\section{Discussion}\label{sec6}

Our systematic exploration of ensemble strategies for combining Hydra and Quant reveals both the potential and limitations of targeted ensemble approaches in time series classification. We achieved measurable improvements (mean 0.72\% gain over best base), demonstrating that complementary algorithms from different paradigms can be successfully combined. However, the modest magnitude of gains and low oracle utilization (11.0\% on average) indicate substantial room for improvement in meta-learning strategies.

\subsection{Implications of Oracle Potential as Primary Predictor}

The moderate correlation between oracle gain and ensemble success ($r=0.631$, $p=0.050$) suggests that prediction-level complementarity provides an upper bound on potential gains, though the relationship is not deterministic. This finding challenges the common practice of combining algorithms based primarily on representational diversity. While our results confirm that Hydra and Quant do exhibit complementary features (median cross-correlation 0.46--0.71 across datasets), this feature-level complementarity does not predict ensemble gains ($r=0.328$, $p=0.36$).

Importantly, the oracle bound constrains only prediction-combination strategies, not feature-concatenation approaches. Our empirical results (Section 5.1) demonstrate that FC-ExtraTrees exceeded the oracle bound on all 10 datasets by correctly predicting 12,556 samples where both base algorithms failed. This confirms the theoretical prediction that joint feature-space models can learn interactions invisible to individual classifiers, though the magnitude of oracle-exceeding gains remained modest (0.21--3.40\% of test samples across datasets).

The practical implication is to shift ensemble design focus from "What different representations do these algorithms use?" to "Do these algorithms make mistakes on different samples?" The oracle analysis provides a simple diagnostic: if oracle gain is small (for example, LakeIce with 0.14\% oracle gain), ensemble methods are unlikely to provide substantial improvements regardless of how complementary the features appear. Conversely, high oracle gain (USCActivity with 12.4\%) indicates fertile ground for ensemble methods, even if feature representations show moderate correlation.

This principle extends beyond Hydra-Quant combinations. For any pair of candidate algorithms, computing oracle accuracy on a validation set provides an upper bound estimate before committing computational resources to training complex meta-learners or exploring ensemble architectures. When oracle potential is low, practitioner effort may be better spent on algorithm selection or hyperparameter tuning rather than ensemble construction.

\subsection{The Meta-Learning Optimization Gap}

Our ensembles captured only 11.0\% of theoretical oracle potential, indicating that current meta-learning strategies are far from optimal. This raises a fundamental question: why do meta-learners struggle to exploit complementarity that oracle analysis shows exists?

We identify three potential explanations. First, \textbf{insufficient meta-learning signal}: the meta-learner trains on $n$ OOF predictions (where $n$ is training set size), which may be inadequate for learning reliable combination rules, particularly for datasets with many classes where the prediction space is high-dimensional. Second, \textbf{information bottleneck}: the 2$c$-dimensional logit space (for $c$ classes) may not encode sufficient information about when each algorithm should be trusted. The confidence scores reflect how certain each algorithm is, but not the problem characteristics that determine reliability. Third, \textbf{meta-learner capacity}: ExtraTrees may lack the capacity to learn complex conditional logic like "trust Hydra on long-series problems with periodic patterns, but trust Quant on short-series problems with distributional shifts."

Addressing these limitations suggests several research directions. \textbf{Richer meta-features}: Rather than using only logits, meta-learners could incorporate problem descriptors (series length, number of classes, distribution statistics) to learn context-dependent combination rules. \textbf{Instance-level features}: Computing simple statistics on the input time series (mean, variance, autocorrelation) and concatenating with logits may help the meta-learner identify when each algorithm should be trusted. \textbf{Metalearning architectures}: Neural meta-learners or gradient-boosted trees might better capture complex combination logic than ExtraTrees. \textbf{Multi-level stacking}: Training multiple meta-learner layers, where higher levels learn when to trust lower-level predictions, could iteratively refine combination strategies.

However, these directions trade increased complexity for potentially marginal gains. Given that our simplest successful configuration (QFeat-HLogit-ET) achieves only 11.0\% oracle utilization with modest actual gains (0.72\%), the question becomes: is the computational overhead of more sophisticated meta-learning justified? For practitioners working with limited computational budgets, focusing on algorithm selection and hyperparameter tuning may yield better returns than elaborate ensemble architectures.

\subsection{No Universal Ensemble Strategy}

A critical finding is that no single ensemble configuration dominates across all datasets. QFeat-HLogit-ET ranks first on 4 of 10 datasets, CAWPE on 3, DualOOF-ET on 1, with base algorithms winning the remaining 2. This heterogeneity mirrors the broader TSC principle that "no one representation will dominate" \citep{tsc-bakeoff}, extended to the meta-learning level.

This result has important implications. First, it suggests that \textbf{ensemble selection is itself a meta-learning problem}: given a dataset, which combination strategy (feature concatenation, asymmetric stacking, symmetric stacking, or weighted averaging) will perform best? Our experiments do not identify reliable predictors of which strategy succeeds, as dataset properties (size, classes, length) showed no significant correlations with ensemble choice effectiveness.

Second, the heterogeneity validates our systematic design space exploration approach. Had we tested only one ensemble configuration (as prior work often does), we would have drawn misleading conclusions about Hydra-Quant complementarity based on which specific configuration happened to be tested. The finding that ExtraTrees universally outperforms Ridge for feature concatenation, while different high-level strategies (stacking vs weighted averaging) excel on different datasets, suggests that meta-learner choice and combination strategy operate at different levels of the design space.

Third, for practitioners, this implies that \textbf{cross-validation over ensemble configurations} may be necessary. Rather than selecting one ensemble strategy a priori, allocating computational budget to evaluate 2 to 3 configurations via nested cross-validation may improve final performance. However, this compounds the already substantial computational cost: each stacking ensemble requires 5-fold cross-validation, and evaluating multiple strategies multiplies this overhead.

\subsection{Computational Cost Considerations}

Our strongest ensemble (QFeat-HLogit-ET) provides a mean 0.72\% accuracy improvement while increasing training time by approximately 2.5 times (from Quant alone) due to 5-fold cross-validation overhead. For practitioners, this trade-off may be unfavorable in many contexts. If computational budget is limited, running Quant (which individually outperforms Hydra on 8 of 10 datasets) may be preferable to investing resources in ensemble construction.

The cost-benefit analysis depends on problem context. For applications where small accuracy improvements are valuable (medical diagnosis, financial prediction), the 0.72\% gain may justify increased computational cost. For rapid prototyping or large-scale benchmarking studies, the computational overhead may be prohibitive. Figure~\ref{fig:computational} (Appendix~\ref{appE}) shows that Quant and Hydra provide the best efficiency-accuracy trade-offs, with ensembles falling in the high-cost, modest-benefit region.

This suggests a staged approach: begin with individual base algorithms to establish baselines, compute oracle gain to assess ensemble potential, and only if oracle gain exceeds a threshold (for example, 5\%) invest in ensemble construction. This avoids wasted effort on datasets where theoretical potential is minimal.

\subsection{Limitations and Future Work}

Our study focuses on two algorithms (Hydra and Quant) from specific paradigms (convolution and interval-based). Generalization to other algorithm pairs requires validation. While the systematic design space exploration and oracle analysis framework applies broadly, the specific finding that QFeat-HLogit-ET performs best may not transfer to other algorithm combinations.

The evaluation uses 10 MONSTER datasets, providing coverage across domains and scales but representing a limited sample of the broader TSC problem landscape. Extension to the full MONSTER collection (29 datasets) and traditional UCR archive would strengthen conclusions about ensemble effectiveness patterns.

Several promising research directions emerge from this work. First, investigating why meta-learners achieve low oracle utilization could lead to improved combination strategies. Second, developing predictive models for ensemble selection (which configuration to use for a given dataset) would automate a currently manual design choice. Third, extending the complementarity analysis framework to larger ensembles (3 or more algorithms) could reveal whether pairwise complementarity predicts multi-algorithm ensemble success. Finally, exploring neural meta-learners that jointly learn feature extraction and combination could surpass the two-stage approach we employ.

Despite these limitations, our work provides the most comprehensive analysis to date of targeted ensemble strategies for modern efficient TSC algorithms, demonstrating both the potential and current limitations of this approach.

\clearpage

\section{Conclusion}\label{sec13}

This work systematically investigated whether targeted ensemble strategies can capture the accuracy benefits of comprehensive multi-paradigm combination while maintaining computational efficiency for large-scale time series classification. We combined two efficient algorithms from distinct paradigms (Hydra's competing convolutional kernels and Quant's hierarchical interval quantiles) across six ensemble configurations, evaluating performance on 10 large-scale MONSTER datasets with training sets ranging from 7,898 to 1,168,774 instances.

Our findings demonstrate that complementary efficient algorithms can be successfully combined to achieve modest but consistent improvements. The strongest configuration, QFeat-HLogit-ET (asymmetric stacking with ExtraTrees meta-learner), achieved mean accuracy of 0.836 compared to 0.829 for the best base algorithm, representing a 0.72\% improvement while improving on 7 of 10 datasets. Feature concatenation with non-linear meta-learning proved essential: ExtraTrees achieved 0.842 mean accuracy versus 0.798 for Ridge regression on the same concatenated features, demonstrating that linear models underfit on these larger problems. Among combination strategies, no single approach dominated. QFeat-HLogit-ET ranked first on 4 datasets, CAWPE on 3, and DualOOF-ET on 1, mirroring the broader TSC principle that algorithm effectiveness is problem-dependent.

However, these gains fall far short of theoretical potential. Oracle analysis revealed that perfect combination could achieve 4.71\% mean improvement over the better base algorithm, yet actual ensembles captured only 11.0\% of this potential on average. This optimization gap indicates that current meta-learning strategies struggle to reliably identify when to trust each algorithm's predictions. The moderate correlation between oracle gain and ensemble success ($r=0.631$, $p=0.050$) approaches but does not reach statistical significance, showing considerable scatter that suggests factors beyond oracle potential (such as meta-learner capacity and feature interactions) play crucial roles. Contrary to expectations, feature-level complementarity showed no significant correlation with ensemble gains ($r=0.328$, $p=0.36$), revealing that complementary representations alone do not guarantee success.

These findings carry practical implications for ensemble design in time series classification. First, practitioners should assess prediction-level complementarity (oracle potential) before investing in complex meta-learning architectures, as this sets an upper bound on achievable gains. When one algorithm dominates (such as Quant achieving 99.7\% on LakeIce), even highly complementary features yield minimal ensemble improvements. Second, meta-learner choice matters: non-linear models like ExtraTrees significantly outperform linear approaches when combining heterogeneous features from different paradigms. Third, the low oracle utilization (11\%) indicates that ensemble gains could potentially be doubled or tripled through improved meta-learning strategies, though achieving this requires solving fundamental challenges in learning when to trust each algorithm.

This work has several limitations that future research should address. First, our CAWPE implementation uses training set accuracy rather than proper cross-validation for component weighting, which may overestimate algorithm reliability. Future work should validate findings with rigorous cross-validation-based weighting. Second, the oracle correlation approaching marginal significance ($p=0.050$) warrants investigation on larger dataset collections to establish whether the relationship strengthens with more samples. Third, we explored only six ensemble configurations from a vast design space; adaptive weighting schemes, neural meta-learners, and instance-level feature augmentation may capture more oracle potential. Fourth, our analysis focuses on two specific algorithms (Hydra and Quant); the principles may generalize to other efficient algorithm pairs, but empirical validation is needed.

Future work should pursue several promising directions. Richer meta-features that incorporate problem descriptors (series length, class count, distributional statistics) may help meta-learners learn context-dependent combination rules, potentially explaining the scatter in our oracle gain correlation. Instance-level feature augmentation, where simple time series statistics (mean, variance, autocorrelation) are concatenated with algorithm logits, could provide meta-learners with crucial information about when each algorithm should be trusted. More sophisticated meta-learning architectures, such as neural networks or gradient-boosted trees, may have the capacity to learn the complex conditional logic that ExtraTrees appears to lack. Finally, extending this analysis to the full MONSTER archive (30 datasets) and investigating whether ensemble effectiveness patterns differ between UCR and MONSTER would clarify how dataset scale affects combination strategies.

The central insight from this work is that the challenge in ensemble time series classification has shifted from ensuring algorithms are different to learning how to combine them effectively. Hydra and Quant exhibit clear complementarity: moderate feature correlation (0.46 to 0.71), substantial error independence (average error correlation 0.421), and meaningful oracle potential (mean 4.71\%). Yet current meta-learning captures only 11\% of this potential. This gap represents both a limitation of current approaches and an opportunity for future research. As time series classification confronts increasingly large-scale real-world problems, developing efficient ensembles that effectively exploit complementarity between fast algorithms becomes not just an academic exercise, but a practical necessity. 

\backmatter

\clearpage

\begin{appendices}

\section{Complete Algorithm Comparison}\label{appA}

Table~\ref{tab:full_results} presents complete results across all ensemble configurations.

\input{tableA1_full_results}

\clearpage

\section{Dataset Characteristics}\label{appB}

Table~\ref{tab:dataset_characteristics} summarizes characteristics of the 10 MONSTER benchmark datasets used in this study.

\input{tableA2_dataset_characteristics}

\clearpage

\section{Complementarity Metrics}\label{appC}

Table~\ref{tab:complementarity_summary} presents complete complementarity statistics and ensemble performance.

\input{table2_complementarity_summary}

Table~\ref{tab:complementarity_summary} shows substantial variation in feature-level complementarity (Feat Corr), prediction-level complementarity (Err Corr), oracle potential (Oracle Gain), and actual ensemble improvements across datasets. InsectSound, WISDM, and USCActivity exhibit the highest oracle gains (7.7\%, 4.7\%, 12.4\% respectively) and correspondingly achieve the largest ensemble improvements. This pattern demonstrates that oracle potential (the theoretical ceiling) predicts ensemble success better than feature-level complementarity measures.

\clearpage

\section{Complete Algorithm Comparison}\label{appD}

Figure~\ref{fig:full_heatmap} shows the complete algorithm comparison heatmap across all 9 datasets (Traffic excluded due to missing ensemble configuration).

\begin{figure*}[!htb]
\centering
\includegraphics[width=\textwidth]{figureA1_full_heatmap}
\caption{Complete algorithm comparison across 9 MONSTER datasets (Traffic excluded). Colors indicate performance relative to best base algorithm per dataset. Green cells show improvements, red shows degradation, yellow shows comparable performance. No single ensemble dominates across all datasets, validating the need for problem-specific ensemble selection.}\label{fig:full_heatmap}
\end{figure*}

\clearpage

\section{Computational Cost Analysis}\label{appE}

Figure~\ref{fig:computational} presents computational cost analysis across all algorithms.

\begin{figure*}[!htb]
\centering
\includegraphics[width=\textwidth]{figureA2_computational_cost}
\caption{Computational cost analysis. (A) Training time distribution per algorithm showing median (orange line) and quartiles. Ensembles with OOF prediction generation (QFeat-HLogit-ET, CAWPE, DualOOF-ET) incur 5-fold cross-validation overhead. (B) Accuracy vs normalized computational cost (time per 1000 training samples). Normalization accounts for dataset size differences. Quant and Hydra-Multi offer best efficiency, while ensembles trade computational cost for marginal accuracy gains.}\label{fig:computational}
\end{figure*}

\end{appendices}

\clearpage

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgements}

Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

Please refer to Journal-level guidance for any specific requirements.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval and consent to participate
\item Consent for publication
\item Data availability 
\item Materials availability
\item Code availability 
\item Author contribution
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\clearpage

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

\end{document}
