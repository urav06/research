%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}        % Style for submissions to Nature Portfolio journals
\documentclass[pdflatex,sn-basic]{sn-jnl}           % Basic Springer Nature Reference Style/Chemistry Reference Style
%%\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}  % Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}   % Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}           % American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl} % Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}  % Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}           % APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}       % Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}               %  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]    % meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}     % to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Article Title}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}

\author[2,3]{\fnm{Second} \sur{Author}}\email{iiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

\affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

\affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. Authors are advised to check the author instructions for the journal they are submitting to for word limits and if structural elements like subheadings, citations, or equations are permitted.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{keyword1, Keyword2, Keyword3, Keyword4}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.

\section{Related Work}\label{sec2}

The evolution of time series classification has been characterized by the emergence of diverse algorithmic paradigms, each capturing different aspects of temporal patterns, alongside sophisticated ensemble methods that achieve state-of-the-art accuracy through principled combination strategies. In this section, we examine the theoretical foundations underlying successful TSC approaches, ensemble design principles, and identify opportunities for targeted algorithmic complementarity that balance accuracy with computational efficiency.

\subsection{Time Series Classification Paradigms}

The comprehensive evaluation by \citet[p.~610]{tsc-bakeoff} established a fundamental taxonomic framework for TSC algorithms, identifying five primary paradigms based on their discriminatory features. \textbf{Whole series methods} compare time series as vectors or using elastic distance measures, with Dynamic Time Warping serving as the canonical example. \textbf{Interval-based methods} select phase-dependent segments of the series, exemplified by Time Series Forest \cite{deng-2013} which extracts features from random intervals. \textbf{Shapelet-based approaches} discover phase-independent discriminative subsequences \cite{time_series_shapelets}, while \textbf{dictionary methods} build classification models on histograms of recurring pattern frequencies \cite{schaefer-2015}. Finally, \textbf{combination approaches} integrate multiple paradigms to leverage complementary information sources.

This paradigm diversity reflects a fundamental principle established by \citet[p.~637]{tsc-bakeoff}: ``no one representation will dominate'' across the diverse range of problem types encountered in practice. Their rigorous evaluation of 18 algorithms across 85 datasets revealed that performance differences between optimal and suboptimal paradigms can range from 5-15\% \citep[p.~647, Table~11]{tsc-bakeoff}, demonstrating substantial gains achievable through intelligent paradigm combination.

\subsection{Ensemble Methods in Time Series Classification}

\subsubsection{Theoretical Foundations}

The theoretical foundation for ensemble methods in machine learning was established by \citet[p.~241]{stacked-generalization}, who demonstrated that ensemble approaches work by ``deducing the biases of the generalizer(s) with respect to a provided learning set.'' Wolpert's stacked generalization framework provides a principled meta-learning approach that goes beyond simple voting by learning optimal combination weights through structured bias correction \citep[p.~242]{stacked-generalization}.

Modern ensemble theory emphasizes the critical role of diversity in achieving performance gains. The bias-variance-covariance decomposition shows that ensemble effectiveness depends not only on individual component accuracy but also on the correlation structure between base learners. For time series classification, this diversity principle manifests through the combination of different representational paradigms that capture orthogonal aspects of temporal patterns.

\subsubsection{Hierarchical Ensemble Architecture}

The first comprehensive ensemble approach in TSC was COTE (Collective of Transformation-based Ensembles), which provided empirical validation of paradigm diversity principles \citep[p.~609]{tsc-bakeoff}. By combining four distinct transformation domains (shapelet, dictionary, interval, and spectral), COTE achieved over 8\% improvement compared to the then-dominant DTW approach, demonstrating the substantial gains possible through paradigm integration.

HIVE-COTE extended this approach through hierarchical architecture that addressed bias issues in flat ensemble structures \citep[p.~7]{hive-cote}. The key innovation was organizing ensembles into domain-specific modules, each providing a single weighted vote rather than allowing domains with more base learners to dominate the final decision. HIVE-COTE 2.0 \citep[p.~3]{hive-cote-2} further refined this architecture with five domains: dictionary-based TDE, interval-based DrCIF, convolution-based Arsenal, shapelet-based STC, and distance-based methods, achieving state-of-the-art accuracy through this comprehensive coverage.

\subsubsection{Ensemble Combination Strategies}

Beyond architectural considerations, the method for combining ensemble predictions critically impacts performance. \citet[p.~3]{cawpe} introduced the Cross-validation Accuracy Weighted Probabilistic Ensemble (CAWPE) framework, which uses exponential weighting with parameter $\alpha=4$ to ``magnify differences in competence'' between base classifiers. The mathematical formulation $p(y=i|E,x) \propto \sum_{j=1}^k w_j^4 p_j(y=i|M_j,x)$ \citep[p.~8, Equation~1]{cawpe} significantly outperforms simple averaging or voting strategies.

The effectiveness of probabilistic weighting over prediction-based combination was demonstrated through comprehensive ablation studies \citep[p.~22, Figure~12]{cawpe}, showing consistent improvements across accuracy, AUC, and negative log-likelihood metrics. When applied to HIVE-COTE, CAWPE improved overall accuracy from 85.97\% to 87.16\% \citep[p.~19]{cawpe}, representing a substantial 1.2 percentage point gain through improved combination methodology alone.

Recent advances in ensemble combination include possibilistic approaches based on conformal prediction theory. \citet[p.~4]{ensemble-predictors} introduced Ensemble Predictors that use three combination rules: $\Gamma_{min}$, $\Gamma_{max}$, and $\Gamma_w$, each with different validity guarantees. The $\Gamma_{max}$ predictor provides conservative validity with $P_r(y \notin \Gamma_{max}^{\varepsilon}) = C_Q(\varepsilon,\ldots,\varepsilon) \leq \varepsilon$ \citep[Theorem~4, p.~5]{ensemble-predictors}, offering theoretical performance bounds absent from traditional ensemble approaches.

\subsection{Efficient Algorithm Development}

\subsubsection{The Convolutional Kernel Revolution}

A paradigm shift occurred with ROCKET \citep[p.~1455]{rocket}, which demonstrated that ``simple linear classifiers using random convolutional kernels achieve state-of-the-art accuracy with a fraction of the computational expense of existing methods.'' ROCKET's innovation challenged the prevailing assumption that computational complexity was necessary for high accuracy in TSC, achieving competitive performance with HIVE-COTE while requiring less than 2 hours for training across all 85 UCR datasets compared to days for existing methods.

The foundation enabled rapid algorithmic development within the ROCKET family. MiniROCKET \cite{minirocket} achieved near-deterministic performance with 75x speed improvements through binary kernel weights and bias sampling strategies. MultiROCKET \cite{multirocket} extended the approach with multiple pooling operators and first-order difference transformations, demonstrating that computational efficiency and representational diversity are not mutually exclusive objectives.

\subsubsection{Targeted Algorithmic Specialization}

Building on ROCKET's computational efficiency principles, recent work has explored targeted algorithms designed for specific representational strengths. \citet[p.~1780]{hydra} introduced Hydra, which bridges dictionary and convolutional approaches through ``competing convolutional kernels'' organized into groups where only the closest pattern match contributes to feature representation. This design combines ROCKET's computational efficiency with dictionary methods' interpretability advantages, achieving accuracy comparable to several state-of-the-art methods while maintaining fast training times.

Simultaneously, \citet[p.~2377]{quant} developed Quant, representing an ``attempt to condense various [interval-based] approaches into their essential ingredients.'' Quant's theoretical foundation rests on the principle that ``class can be distinguished based on the distribution of values in different locations of a time series,'' providing a complementary perspective to pattern-based approaches. Using only quantile features and fixed dyadic intervals, Quant achieves state-of-the-art accuracy with total compute time of less than 15 minutes across 142 UCR datasets.

\subsection{Complementarity and Meta-Learning}

\subsubsection{Algorithm Selection and Meta-Learning}

The challenge of selecting appropriate TSC algorithms for specific problems has motivated meta-learning approaches that leverage historical performance patterns. The TSC-AutoML framework \citep{tsc-automl} addresses the Combined Algorithm Selection and Hyperparameter optimization (CASH) problem through a dual insurance strategy combining experience-driven meta-learners with warm-start optimization. Using 32 meta-features across time, frequency, and statistical domains \citep[Table~I, p.~8]{tsc-automl}, the approach achieves 68\% top-1 algorithm selection accuracy compared to 8-28\% for baseline AutoML methods \citep[Table~IV, p.~10]{tsc-automl}.

The meta-learning perspective provides crucial insights for ensemble design by revealing which algorithmic approaches excel under different problem characteristics. The systematic categorization of TSC algorithms into complementary families \citep[Table~II, p.~9]{tsc-automl} demonstrates that ``no machine learning algorithm will ever be superior to any other algorithm in every aspect and tasks,'' reinforcing the theoretical basis for ensemble approaches that combine diverse algorithmic strengths.

\subsubsection{Complementarity Analysis Frameworks}

Understanding algorithmic complementarity requires systematic analysis of when and why different approaches succeed. The paradigm diversity principle established by \citet[p.~637]{tsc-bakeoff} provides empirical evidence for complementarity through cross-paradigm performance analysis, showing that algorithms optimized for one problem type may perform 5-15\% worse on problems suited to different paradigms \citep[Table~11, p.~647]{tsc-bakeoff}.

Modern approaches to complementarity analysis leverage comprehensive feature characterizations to understand algorithm-problem relationships. The systematic extraction of 794 time-series features using established libraries enables principled analysis of which problem characteristics favor different algorithmic approaches, providing a foundation for targeted ensemble construction that goes beyond ad-hoc combination strategies.

\subsection{Evaluation Methodology and Computational Considerations}

\subsubsection{Dataset Selection and Benchmarking Bias}

Traditional TSC evaluation has relied heavily on the UCR archive, but recent work has identified critical limitations in this approach. \citet[p.~5]{less-is-more} demonstrated that the UCR benchmark fails key quality requirements for non-redundancy, with clusters containing highly uneven dataset distributions (65, 53, 7, 68, 55, and 1 unique datasets respectively). This bias means that ``including more datasets from the same part of the landscape is in favor of some of the algorithms and the statistical outcome is questionable'' \citep[p.~6]{less-is-more}.

The solution involves landscape analysis using comprehensive feature extraction (794 tsfresh features \citep[p.~3]{less-is-more}) followed by uniform sampling from identified problem clusters rather than using all available datasets. Bootstrap validation with 30 repetitions and 90\% representativeness thresholds \citep[p.~5-6]{less-is-more} provides more robust statistical comparisons than traditional approaches.

\subsubsection{Large-Scale Evaluation Frameworks}

The computational challenge of comprehensive ensemble evaluation, exemplified by HIVE-COTE 2.0's requirement of 340+ hours for complete training \citep[Table~4, p.~17]{hive-cote-2}, necessitates careful consideration of evaluation scalability. While contractable training achieves 98\% of full accuracy in 4 hours and 99\% in 12 hours \citep[p.~27]{hive-cote-2}, the fundamental tension between comprehensive coverage and computational efficiency remains unresolved.

This creates an opportunity for targeted ensemble approaches that achieve comparable accuracy through strategic algorithm selection rather than exhaustive paradigm coverage, particularly when evaluated on modern large-scale benchmarks that better reflect real-world computational constraints and data volumes.

\section{Method}\label{sec3}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.

\section{Experimental Setup}\label{sec4}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.

\section{Results}\label{sec5}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.

\section{Discussion}\label{sec6}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.

\section{Conclusion}\label{sec13}

Conclusions may be used to restate your hypothesis or research question, restate your major findings, explain the relevance and the added value of your work, highlight any limitations of your study, describe future directions for research and recommendations. 

In some disciplines use of Discussion or 'Conclusion' is interchangeable. It is not mandatory to use both. Please refer to Journal-level guidance for any specific requirements. 

\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgements}

Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

Please refer to Journal-level guidance for any specific requirements.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval and consent to participate
\item Consent for publication
\item Data availability 
\item Materials availability
\item Code availability 
\item Author contribution
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\begin{appendices}

\section{Section title of first appendix}\label{secA1}

An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

\end{document}
